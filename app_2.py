import streamlit as st
import pandas as pd
import requests
import yfinance as yf
import base64
import os
import time
from PIL import Image
from datetime import datetime, timedelta
from dateutil import parser as date_parser
from datetime import timezone


st.set_page_config(layout="wide")

st.markdown(
    """
    <style>
    /* Lock the sidebar width */
    [data-testid="stSidebar"] {
        background-color: #D5EDF8 !important;
    }

    /* Prevent resizer from showing on hover */
    [data-testid="stSidebar"] + div [data-testid="stResizer"] {
        display: none !important;
    }

    /* Sidebar pills/buttons */
    .st-emotion-cache-1avcm0n, .st-emotion-cache-16txtl3 {
        background-color: #00C2D6 !important;
        color: white !important;
        font-weight: bold;
    }

    /* Header */
    h1 {
        color: #102A43;
    }

    /* Global font */
    html, body, [class*="css"]  {
        font-family: 'Segoe UI', sans-serif;
        font-size: 15px;
        line-height: 1.6;
    }

    /* Sidebar "Select a page" label */
    [data-testid="stSidebar"] label {
        color: #000000 !important;
        font-weight: bold;
    }

    /* Sidebar selectbox/dropdown styling */
    [data-testid="stSidebar"] .stSelectbox div[data-baseweb="select"] {
        background-color: #102A43 !important;
        color: #EEEFF3 !important;
        border-radius: 5px;
    }

    /* Input text (search box inside dropdown) */
    [data-testid="stSidebar"] input {
        color: #EEEFF3 !important;
    }

    /* Dropdown list background */
    [data-testid="stSidebar"] .stSelectbox [role="listbox"] {
        background-color: #102A43 !important;
        color: #EEEFF3 !important;
    }

    /* GLOBAL FLAT CORNER STYLE */
    button, input, select, textarea,
    div[data-baseweb="select"],
    .stButton > button,
    .stTextInput > div,
    .stSelectbox > div,
    .stMultiSelect > div,
    .stDateInput > div,
    .stRadio > div,
    .stSlider > div {
        border-radius: 4px !important;  /* Use 0px for square, 4px for slight curve */
        box-shadow: none !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

NEWS_API_KEY = st.secrets.get("NEWS_API_KEY", "")
TWITTER_BEARER_TOKEN = st.secrets.get("TWITTER_BEARER_TOKEN", "")

SEC_FACTS = {
    "Bitcoin Held": [
        "us-gaap:CryptoAssetNumberOfUnits"
    ],
    "Revenue": [
        "us-gaap:Revenues",
        "us-gaap:RevenueFromContractWithCustomerIncludingAssessedTax"
    ],
    "General and Administrative Expense": [
        "us-gaap:GeneralAndAdministrativeExpense"
    ],
    "Cost of Revenue": [
        "us-gaap:CostOfRevenue",
        "us-gaap:CostsAndExpenses"
    ],
    "Cash & Equivalents": [
        "us-gaap:CashAndCashEquivalentsAtCarryingValue"
    ],
    "Earnings Per Share (Basic)": [
        "us-gaap:EarningsPerShareBasic",
        "us-gaap:IncomeLossFromContinuingOperationsPerBasicShare",
        "us-gaap:BusinessAcquisitionProFormaEarningsPerShareBasic"
    ],
    "Earnings Per Share (Diluted)": [
        "us-gaap:EarningsPerShareDiluted",
        "us-gaap:IncomeLossFromContinuingOperationsPerDilutedShare",
        "us-gaap:BusinessAcquisitionProFormaEarningsPerShareDiluted"
    ]
}

range_options = {
    "1 Day": "1d", "1 Week": "5d", "1 Month": "1mo", "6 Months": "6mo", "1 Year": "1y"
}

competitor_tickers = ["MARA", "RIOT", "CIFR", "HUT", "BITF"]

# --- Helper Functions ---

def format_timestamp(iso_string):
    dt = date_parser.parse(iso_string).astimezone(timezone.utc)
    now = datetime.now(timezone.utc).replace(tzinfo=timezone.utc)
    delta = now - dt

    # Format like "June 18, 12:34 PM"
    nice_time = dt.strftime("%B %d, %I:%M %p").lstrip("0").replace(" 0", " ")

    # Format relative time
    seconds = int(delta.total_seconds())
    if seconds < 60:
        relative = f"{seconds} sec ago"
    elif seconds < 3600:
        minutes = seconds // 60
        relative = f"{minutes} min ago"
    elif seconds < 86400:
        hours = seconds // 3600
        relative = f"{hours} hour{'s' if hours != 1 else ''} ago"
    else:
        days = seconds // 86400
        relative = f"{days} day{'s' if days != 1 else ''} ago"

    return f"{nice_time} ({relative})"

@st.cache_data(ttl=300)  # Cache for 5 minutes
def get_coingecko_btc_data():
    url = "https://pro-api.coingecko.com/api/v3/simple/price"
    params = {
        "ids": "bitcoin",
        "vs_currencies": "usd",
        "include_market_cap": "true",
        "include_24hr_vol": "true",
        "include_24hr_change": "true"
    }
    headers = {
        "x-cg-pro-api-key": st.secrets["COINGECKO_API_KEY"]
    }

    try:
        resp = requests.get(url, params=params, headers=headers)
        resp.raise_for_status()
        data = resp.json()["bitcoin"]
        return {
            "price": data["usd"],
            "market_cap": data["usd_market_cap"],
            "volume": data["usd_24h_vol"],
            "change": data["usd_24h_change"]
        }
    except Exception as e:
        st.warning("Failed to fetch CoinGecko Pro data.")
        st.text(f"Error: {e}")
        return {}

def get_history(_ticker, period):
    interval = "5m" if period == "1d" else "1d"
    return _ticker.history(period=period, interval=interval)

@st.cache_data(ttl=1800) # 30 minutes
def get_news(query, exclude=None, sort_by="popularity", page_size=10, from_days=30, page=1, domains=None):
    term = f"{query} -{exclude}" if exclude else query
    from_date = (datetime.now() - timedelta(days=from_days)).strftime("%Y-%m-%d")
    
    url = (
        f"https://newsapi.org/v2/everything?q={term}&from={from_date}&sortBy={sort_by}"
        f"&language=en&pageSize={page_size}&page={page}&apiKey={NEWS_API_KEY}"
    )
    
    if domains:
        domain_str = ",".join(domains)
        url += f"&domains={domain_str}"

    try:
        resp = requests.get(url)
        if resp.status_code != 200:
            st.warning(f"NewsAPI Error {resp.status_code}: {resp.json().get('message', 'Unknown error')}")
            return []
        return resp.json().get("articles", [])
    except Exception as e:
        st.error(f"NewsAPI request failed: {e}")
        return []


def regulatory_article_filter(article):
    t = (article.get("title") or "").strip().lower()
    d = (article.get("description") or "").strip().lower()
    content = f"{t} {d}"
    crypto_terms = [
        "crypto", "cryptocurrency", "bitcoin", "ethereum", "stablecoin",
        "tokenized", "defi", "blockchain", "digital asset", "web3"
    ]
    if not any(term in content for term in crypto_terms):
        return False
    regulatory_terms = [
        "sec", "cftc", "irs", "treasury", "white house", "congress",
        "senate", "legislation", "bill", "law", "regulator", "oversight",
        "framework", "compliance", "approval", "vote", "policy", "rules"
    ]
    return any(term in content for term in regulatory_terms)

def load_articles(key, query, exclude=None, from_days=30, sort_by="popularity", filter_func=None):
    PAGE_SIZE = 10
    batch = get_news(query, exclude, sort_by, PAGE_SIZE, from_days, page=1)
    if filter_func:
        batch = [a for a in batch if filter_func(a)]
    st.session_state[key] = batch[:PAGE_SIZE]
    for art in st.session_state[key]:
        st.markdown(f"**[{art['title']}]({art['url']})**")
        st.caption(f"Source: {art['source']['name']} | {art['publishedAt'][:10]}")

@st.cache_data(ttl=1800)
def get_cleanspark_tweets(query_scope="CleanSpark", max_age_days=1, sort_by="likes", max_results=6):
    headers = {"Authorization": f"Bearer {st.secrets['TWITTER_BEARER_TOKEN']}"}

    if query_scope == "CleanSpark":
        query = '("CleanSpark" OR #CLSK) -is:retweet has:links'
    else:
        query = '(bitcoin OR BTC OR mining OR crypto) -is:retweet has:links'

    from_date = (datetime.utcnow() - timedelta(days=max_age_days)).strftime("%Y-%m-%dT%H:%M:%SZ")

    url = "https://api.twitter.com/2/tweets/search/recent"
    params = {
        "query": query,
        "max_results": 20,
        "tweet.fields": "public_metrics,created_at,author_id,entities",
        "start_time": from_date,
        "expansions": "attachments.media_keys,author_id",
        "media.fields": "url,preview_image_url,type",
        "user.fields": "username,name,profile_image_url"
    }

    # Retry logic
    for attempt in range(3):
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 429:
            st.warning("Twitter rate limit hit. Retrying in 60 seconds...")
            time.sleep(60)
        else:
            break

    if response.status_code != 200:
        st.error(f"Twitter API Error: {response.status_code} ‚Äî {response.text}")
        return []

    data = response.json()
    tweets = data.get("data", [])
    users = {u["id"]: u for u in data.get("includes", {}).get("users", [])}
    media_map = {m["media_key"]: m for m in data.get("includes", {}).get("media", [])}

    # Build full tweet results
    results = []
    for tweet in tweets:
        user = users.get(tweet["author_id"])
        media_urls = [
            media_map[m].get("url") or media_map[m].get("preview_image_url")
            for m in tweet.get("attachments", {}).get("media_keys", []) if m in media_map
        ]
        results.append({
            "text": tweet["text"],
            "username": user["username"] if user else "",
            "name": user["name"] if user else "",
            "profile_img": user.get("profile_image_url", "") if user else "",
            "created_at": tweet["created_at"],
            "likes": tweet["public_metrics"]["like_count"],
            "retweets": tweet["public_metrics"]["retweet_count"],
            "tweet_id": tweet["id"],
            "media": media_urls
        })

    # Sort locally
    if sort_by == "likes":
        results.sort(key=lambda x: x["likes"], reverse=True)
    elif sort_by == "retweets":
        results.sort(key=lambda x: x["retweets"], reverse=True)
    else:  # published
        results.sort(key=lambda x: x["created_at"], reverse=True)

    return results[:max_results]

@st.cache_data(ttl=300)
def get_competitor_prices(symbols):
    try:
        data = yf.download(
            tickers=" ".join(symbols),
            period="2d",
            interval="1d",
            group_by="ticker",
            auto_adjust=True,
            progress=False,
            threads=True
        )
    except Exception as e:
        st.error(f"Failed to fetch batch stock data: {e}")
        return []

    results = []

    for sym in symbols:
        try:
            df = data[sym] if len(symbols) > 1 else data
            if df.empty:
                continue

            if len(df) >= 2:
                prev_close = df["Close"].iloc[-2]
                latest_close = df["Close"].iloc[-1]
            else:
                prev_close = df["Open"].iloc[0]
                latest_close = df["Close"].iloc[0]

            change = ((latest_close - prev_close) / prev_close) * 100

            results.append({
                "symbol": sym,
                "price": latest_close,
                "change": change
            })
        except Exception as e:
            st.warning(f"Error processing {sym}: {e}")
            continue

    return results

@st.cache_data(ttl=900)
def get_latest_sec_fact_with_fallback(cik, tags, year_cutoff=2024, expected_duration=90, tolerance=10):
    cik_clean = str(int(cik)).zfill(10)
    headers = {"User-Agent": "CleanSpark Dashboard <zgibbs@cleanspark.com>"}

    for tag in tags:
        url = f"https://data.sec.gov/api/xbrl/companyconcept/CIK{cik_clean}/{tag.replace(':', '/')}.json"
        try:
            resp = requests.get(url, headers=headers)
            if resp.status_code != 200:
                continue

            data = resp.json()
            units = data.get("units", {})

            for unit, values in units.items():
                filtered = []
                for v in values:
                    val = v.get("val")
                    if val is None:
                        continue

                    end = v.get("end") or v.get("date")
                    try:
                        end_year = int(end[:4])
                    except:
                        continue
                    if end_year < year_cutoff:
                        continue

                    start = v.get("start")
                    if start:
                        try:
                            d_start = date_parser.parse(start)
                            d_end = date_parser.parse(end)
                            duration = (d_end - d_start).days
                            if abs(duration - expected_duration) > tolerance:
                                continue
                        except:
                            continue

                    scale = (v.get("scale") or "").lower()
                    scale_factors = {
                        "thousands": 1_000,
                        "millions": 1_000_000,
                        "billions": 1_000_000_000
                    }
                    val *= scale_factors.get(scale, 1)
                    accn = v.get("accn")
                    filtered.append((val, end, accn))

                if filtered:
                    return sorted(filtered, key=lambda x: x[1], reverse=True)[0]

        except Exception as e:
            print(f"Error fetching {tag}: {e}")

    return None, None, None
    
@st.cache_data(ttl=86400)  # cache for 1 day
def load_cik_map():
    url = "https://www.sec.gov/files/company_tickers.json"
    headers = {"User-Agent": "CleanSpark Dashboard <zgibbs@cleanspark.com>"}
    try:
        resp = requests.get(url, headers=headers)
        data = resp.json()
        # Build a simple lookup: {"MARA": "0001507605", ...}
        return {item["ticker"]: str(item["cik_str"]).zfill(10) for item in data.values()}
    except Exception as e:
        st.error(f"Failed to load SEC ticker data: {e}")
        return {}

cik_map = load_cik_map()

@st.cache_data(ttl=86400)
def get_latest_edgar_inline_url(cik):
    cik_clean = str(int(cik)).zfill(10)
    url = f"https://data.sec.gov/submissions/CIK{cik_clean}.json"
    headers = {"User-Agent": "CleanSpark Dashboard <zgibbs@cleanspark.com>"}
    
    try:
        resp = requests.get(url, headers=headers)
        if resp.status_code != 200:
            return None

        data = resp.json()
        recent = data["filings"]["recent"]
        accession_numbers = recent["accessionNumber"]
        primary_docs = recent["primaryDocument"]
        forms = recent["form"]

        for accn, doc, form in zip(accession_numbers, primary_docs, forms):
            if form in ("10-K", "10-Q") and doc.endswith(".htm"):
                accn_nodash = accn.replace("-", "")
                return f"https://www.sec.gov/ix?doc=/Archives/edgar/data/{int(cik)}/{accn_nodash}/{doc}"
    except Exception as e:
        print("EDGAR inline link fetch failed:", e)

    return None

@st.cache_data(ttl=3600)
def get_latest_press_release_metrics(company_name, ticker_symbol):
    import re

    ticker = ticker_symbol.upper()
    query = f'"{company_name}" earnings OR update OR "quarterly report"'
    articles = get_news(
        query,
        page_size=10,
        sort_by="publishedAt",
        from_days=29,
        domains=["globenewswire.com", "nasdaq.com", "seekingalpha.com", "markets.businessinsider.com"]
    )

    if not articles:
        print("‚ùå No articles returned by NewsAPI.")
    else:
        print(f"‚úÖ Found {len(articles)} articles:")
        for i, art in enumerate(articles):
            print(f"{i+1}. {art.get('title')} ‚Äî {art.get('source', {}).get('name')}")

    # (Optional) Temporarily skip the filter to inspect full list
    article = articles[0]  # Force display of top result for testing

    keywords = ["update", "quarter"]
    article = next(
        (
            a for a in articles
            if (
                ticker in (a.get("title", "") + a.get("description", "")).upper()
                or company_name.upper() in (a.get("title", "") + a.get("description", "")).upper()
            )
        ),
        None
    )

    if not article:
        return None

    title = article.get("title", "")
    description = article.get("description", "")
    content = f"{title} {description}".lower()
    published_date = article.get("publishedAt", "")[:10]
    url = article.get("url", "#")

    metrics = {}

    patterns = {
        "Bitcoin Held": r"(?:held(?: a total of)?|holdings(?: total)?)[^\d]{0,20}([\d,]+)\s*(?:btc|bitcoin)|(?:btc|bitcoin)[^\d]{0,20}([\d,]+)",
        "Revenue": r"revenue(?: (?:of|was))?[\s:]*\$([\d,\.]+)",
        "Cash & Equivalents": r"(?:cash and cash equivalents|cash)(?: (?:totaled|of))?[\s:]*\$([\d,\.]+)",
        "Earnings Per Share (Basic)": r"(?:earnings per share|eps)(?: \(basic\))?[\s:]*\$([\d\.]+)",
        "Earnings Per Share (Diluted)": r"(?:earnings per share|eps)(?: \(diluted\))?[\s:]*\$([\d\.]+)"
    }

    for label, pattern in patterns.items():
        matches = re.findall(pattern, content)

        # Flatten all match groups and keep only digits/decimals
        numbers = [g for pair in matches for g in pair if g.strip().replace(',', '').replace('.', '').isdigit()]

        if numbers:
            try:
                if "Bitcoin" in label:
                    val = max(int(n.replace(",", "")) for n in numbers)
                    metrics[label] = f"{val:,} BTC"
                else:
                    val = max(float(n.replace(",", "")) for n in numbers)
                    metrics[label] = f"${val:,.2f}"
            except ValueError:
                continue

    return {
        "title": title,
        "url": url,
        "date": published_date,
        "metrics": metrics
    }

# --- Tabs and Layout Config ---
st.title("Legal & Market Dashboard")
with st.sidebar:
    st.markdown("<br>", unsafe_allow_html=True)  # spacing

    def image_to_base64(path):
        with open(path, "rb") as f:
            data = f.read()
        return base64.b64encode(data).decode()

    img_b64 = image_to_base64("cleanspark_logo.png")

    st.markdown(
        f"""
        <div style="text-align: center;">
            <a href="https://www.cleanspark.com" target="_blank" title="Visit CleanSpark">
                <img src="data:image/png;base64,{img_b64}" style="width:100%; max-width:400px; border-radius:10px; display:block; margin:auto;" />
            </a>
        </div>
        """,
        unsafe_allow_html=True
    )
tab = st.sidebar.selectbox("Select a page", ["Bitcoin News", "Live Market"])
if tab == "Bitcoin News":
    
    day_options = {
        "1 Day": 1,
        "3 Days": 3,
        "1 Week": 7,
        "1 Month": 30
    }

    sort_by_map = {
        "Popularity": "popularity",
        "Published": "publishedAt",
    }
    btc_metrics = get_coingecko_btc_data()
    btc = yf.Ticker("BTC-USD")

    st.subheader("üìà Bitcoin Market Stats")
    sel = st.pills("Bitcoin price range:", options=list(range_options.keys()), default="1 Day", key="btc_range")
    selected_range = range_options.get(sel, "1mo")
    data = get_history(btc, selected_range)

    if not data.empty:
        m1, m2, m3 = st.columns([.80, 1.20, 1])
        with m1:
            st.metric(
                "Bitcoin Price (USD)",
                f"${btc_metrics.get('price', 0):,.2f}",
                f"{btc_metrics.get('change', 0):.2f}%"
            )
        with m2:
            st.metric(
                "Market Cap",
                f"${btc_metrics.get('market_cap', 0):,.0f}"
            )
        with m3:
            st.metric(
                "24h Volume",
                f"${btc_metrics.get('volume', 0):,.0f}"
            )
        st.line_chart(data["Close"].round(2).rename("Bitcoin Price"))

    col1, col2 = st.columns([1.8,2.2])

    with col1:
        st.subheader("üê¶ Twitter Feed")

        # Row 1: Scope pill (full width)
        tw_scope = st.pills("Tweet Scope:", ["All Bitcoin", "CleanSpark Only"], default="CleanSpark Only", key="tw_scope")

        # Row 2: Time + Sort filters side-by-side
        m1, m2 = st.columns([1, 1])

        with m1:
            tw_days = st.pills("Tweets from the past...", ["1 Day", "3 Days", "1 Week"], default="1 Day", key="tw_days")

        with m2:
            tw_sort = st.pills("Sort tweets by:", ["Likes", "Retweets", "Published"], default="Likes", key="tw_sort")
            
        tw_scope_val = "CleanSpark" if tw_scope == "CleanSpark Only" else "General"
        tw_days_map = {"1 Day": 1, "3 Days": 3, "1 Week": 7}
        tw_max_days = tw_days_map[tw_days]

        tweets = get_cleanspark_tweets(
            query_scope=tw_scope_val,
            max_age_days=tw_max_days,
            sort_by=tw_sort.lower(),  # lowercase for consistency
            max_results=6
        )
        
        for tweet in tweets:
            with st.container():
                st.markdown(f"**[{tweet['name']}](https://twitter.com/{tweet['username']})** ‚Ä¢ @{tweet['username']} ‚Ä¢ *{format_timestamp(tweet['created_at'])}*")
                st.markdown(tweet["text"])
                st.markdown(f"üîÅ {tweet['retweets']} &nbsp;&nbsp;&nbsp; ‚ù§Ô∏è {tweet['likes']}")
                st.markdown(f"[View on Twitter](https://twitter.com/{tweet['username']}/status/{tweet['tweet_id']})")
        
                for url in tweet["media"]:
                    if url.lower().endswith((".jpg", ".png", ".jpeg")):
                        st.image(url, use_container_width=True)
                    elif url.lower().endswith((".mp4", ".mov", ".webm")):
                        st.video(url)
                    else:
                        st.markdown(f"[View Media]({url})")
        
                st.markdown("<hr style='margin: 1rem 0; border: 3.5px solid #D5EDF8;'>", unsafe_allow_html=True)
                    
    # General News
    with col2:
            st.subheader("üì∞ Bitcoin News")

            # Pills for filtering
            scope_options = ["All Bitcoin", "CleanSpark Only", "Regulatory Only"]
            gen_scope = st.pills("Article Scope:", scope_options, default="All Bitcoin", key="news_scope_filter")
    
            gen_col1, gen_col2 = st.columns([1, 1])
            with gen_col1:
                gen_days = st.pills("Articles from the past...", list(day_options.keys()), default="1 Day", key="news_days_filter")
            with gen_col2:
                gen_sort = st.pills("Sort by:", list(sort_by_map.keys()), default="Popularity", key="news_sort_filter")

            # Handle query + filtering logic
            if gen_scope == "CleanSpark Only":
                query_term = "CleanSpark"
                exclude_term = None
                filter_func = None
            elif gen_scope == "Regulatory Only":
                query_term = "GENIUS Act OR cryptocurrency legislation OR crypto bill OR digital asset policy"
                exclude_term = None
                filter_func = regulatory_article_filter
            else:  # All Bitcoin
                query_term = "bitcoin mining"
                exclude_term = "CleanSpark"
                filter_func = None

            # Pass in correct params
            gen_from_days = day_options[gen_days]
            gen_sort_by = sort_by_map[gen_sort]

            load_articles(
                key="gen_articles",
                query=query_term,
                exclude=exclude_term,
                from_days=gen_from_days,
                sort_by=gen_sort_by,
                filter_func=filter_func
            )
                    
# --- HOME TAB ---
if tab == "Live Market":
    btc_metrics = get_coingecko_btc_data()
    btc         = yf.Ticker("BTC-USD")
    col1, col2  = st.columns([2,2])
    sym = st.session_state.get("stock_lookup_ticker", "MARA").strip().upper()
    ticker_obj = yf.Ticker(sym)
    try:
        info = ticker_obj.get_info()
        company_name = info.get("longName", sym)
    except Exception:
        company_name = sym
    # Stock
    with col1:
        st.subheader(f"üìä Stock Market Lookup: {company_name}")        
        m1, m2 = st.columns([1.5, 2.5])
        with m1:
            sym = st.text_input("Stock ticker:", "MARA", key="stock_lookup_ticker").strip().upper()
            cik = cik_map.get(sym.upper())
            entered_ticker = sym.upper()
        
        with m2:
            lookup_range = st.pills("Timeframe:", options=list(range_options.keys()), default="1 Day", key="lookup_range")

        selected_range = range_options.get(lookup_range, "1mo")

        # Fetch data
        ticker_obj = yf.Ticker(sym)

        try:
            info = ticker_obj.get_info()
        except Exception:
            info = {
                "regularMarketPrice": None,
                "open": None,
                "dayHigh": None,
                "dayLow": None,
                "trailingPE": None,
                "fiftyTwoWeekHigh": None,
                "fiftyTwoWeekLow": None,
                "volume": None,
                "averageVolume": None,
                "marketCap": None
            }
            st.warning(f"‚ö†Ô∏è Could not fetch company info for ticker: `{sym}`. Displaying fallback values.")
        try:
            df = ticker_obj.history(period=selected_range, interval="1d")
        except Exception:
            df = None
            st.warning(f"‚ö†Ô∏è No historical data found for `{sym}`.")

        # ROW 2: Show current price on its own line
        price = info.get("regularMarketPrice")
        change_amount = info.get("regularMarketChange")
        change_percent = info.get("regularMarketChangePercent")
        clsk_price = clsk_open = clsk_high = clsk_low = None
        
        if sym != "CLSK":
            clsk = yf.Ticker("CLSK")
            df = ticker_obj.history(period=selected_range, interval="1d")
            try:
                clsk_info = clsk.get_info()
            except Exception:
                clsk_info = {
                    "regularMarketPrice": None,
                    "open": None,
                    "dayHigh": None,
                    "dayLow": None
                }
                st.warning("‚ö†Ô∏è Could not fetch CLSK data. Displaying fallback values.")

            clsk_price = clsk_info.get("regularMarketPrice")
            clsk_open  = clsk_info.get("open")
            clsk_high  = clsk_info.get("dayHigh")
            clsk_low   = clsk_info.get("dayLow")

        # Layout: start with m1 already occupied by st.metric("Current Price")
        m1, m2, m3, m4, m5 = st.columns([1.4, 1.2, 1.2, 1.2, 1.2])

        def render_metric_block(label, value, delta=None, reference=None, show_arrow=False):
            if value is None:
                return f"<div style='font-size:16px; font-weight:600; margin-bottom:4px;'>{label}</div><div>-</div>"

            color = "green" if (delta or 0) >= 0 else "red"
            delta_html = ""

            if delta is not None and reference:
                pct = abs(delta / reference * 100) if reference != 0 else 0
                arrow = "üî∫" if delta >= 0 else "üîª"
                change_text = f"{arrow} ${abs(delta):.2f} ({pct:.1f}%)" if show_arrow else f"${abs(delta):.2f} ({pct:.1f}%)"
                delta_html = f"<div style='color:{color}; font-size:0.85em'>{change_text}</div>"

            return f"""
                <div style='font-size:16px; font-weight:600; margin-bottom:4px;'>{label}</div>
                <div style='font-size:24px; font-weight:bold;'>${value:.2f}</div>
                {delta_html}
            """
        with m1:
            st.markdown(render_metric_block("Current Price", price, delta=change_amount, reference=price, show_arrow=True), unsafe_allow_html=True)

        with m2:
            st.markdown(render_metric_block("CLSK Price", clsk_price), unsafe_allow_html=True)

        with m3:
            st.markdown(render_metric_block("CLSK Open", clsk_open), unsafe_allow_html=True)

        with m4:
            st.markdown(render_metric_block("CLSK High", clsk_high), unsafe_allow_html=True)

        with m5:
            st.markdown(render_metric_block("CLSK Low", clsk_low), unsafe_allow_html=True)

        if "regularMarketPrice" not in info or df.empty:
            st.warning(f"No data available for ticker `{sym}`.")
        else:
            # Format large numbers
            def fmt_float(val): return f"{val:.2f}" if val is not None else "N/A"
            def fmt_millions(val): return f"{val/1_000_000:.2f} M" if val else "N/A"
            def fmt_billions(val): return f"{val/1_000_000_000:.2f} B" if val else "N/A"

            m1, m2, m3 = st.columns(3)
            with m1:
                st.markdown(f"**Open**: {fmt_float(info.get('open'))}")
                st.markdown(f"**High**: {fmt_float(info.get('dayHigh'))}")
                st.markdown(f"**Low**: {fmt_float(info.get('dayLow'))}")
            with m2:
                st.markdown(f"**P/E**: {fmt_float(info.get('trailingPE'))}")
                st.markdown(f"**52wk High**: {fmt_float(info.get('fiftyTwoWeekHigh'))}")
                st.markdown(f"**52wk Low**: {fmt_float(info.get('fiftyTwoWeekLow'))}")
            with m3:
                st.markdown(f"**Vol**: {fmt_millions(info.get('volume'))}")
                st.markdown(f"**Avg Vol**: {fmt_millions(info.get('averageVolume'))}")
                st.markdown(f"**Mkt Cap**: {fmt_billions(info.get('marketCap'))}")
        try:
            interval = "5m" if selected_range == "1d" else "1d"
            df = ticker_obj.history(period=selected_range, interval=interval)
            df.index = pd.to_datetime(df.index)

            if not df.empty and "Close" in df.columns:
                st.line_chart(df["Close"].round(2).rename(f"{sym} Price"))
            else:
                st.warning("No price data available for this range.")
        except Exception as e:
            st.error(f"Error loading chart data: {e}")

        with col2:
            st.subheader("Live Competiton View")
            competitors = get_competitor_prices(competitor_tickers)

            m1, m2, m3, m4, m5 = st.columns(5)
            col_map = [m1, m2, m3, m4, m5]

            for i, comp in enumerate(competitors):
                if i >= len(col_map):
                    break  # safety check if there are fewer than 5 columns defined
                with col_map[i]:
                    arrow = "üî∫" if comp["change"] >= 0 else "üîª"
                    color = "green" if comp["change"] >= 0 else "red"
                    st.markdown(
                        f"""
                        <div style='font-size:16px; text-align: center; padding: 8px 0;'>
                            <strong>{comp['symbol']}</strong><br>
                            ${comp['price']:.2f}<br>
                            <span style='color:{color};'>{arrow} {comp['change']:.2f}%</span>
                        </div>
                        """,
                        unsafe_allow_html=True
                    )

            with col2:
               # --- Combined Filing & Press Metrics ---
                st.subheader("üìä Combined Financial Metrics")

                ticker_upper = sym.upper()
                company_name = info.get("longName") or ticker_upper
                inline_url = get_latest_edgar_inline_url(cik)
                
                # Get latest SEC facts
                facts = {}
                dates = []
                latest_accn = None

                for label, tags in SEC_FACTS.items():
                    val, end_date, accn = get_latest_sec_fact_with_fallback(cik, tags)
                    if end_date:
                        dates.append(end_date)
                    if not latest_accn and accn:
                        latest_accn = accn
                    facts[label] = (val, end_date)

                # Get latest press release data
                company_name = info.get("longName") or ticker_upper
                press_data = get_latest_press_release_metrics(company_name, ticker_upper)

                # --- Combine them ---
                combined_metrics = {}

                # SEC values
                for label, (val, date) in facts.items():
                    if val is not None:
                        combined_metrics[label] = {
                            "value": val,
                            "source": "SEC Filing",
                            "date": date
                        }

                # Press overrides SEC if newer
                if press_data and press_data.get("metrics"):
                    for label, val_str in press_data["metrics"].items():
                        if label not in combined_metrics:
                            combined_metrics[label] = {
                                "value": val_str,
                                "source": "Press Release",
                                "date": press_data.get("date")
                            }
                        else:
                            sec_date = combined_metrics[label]["date"]
                            press_date = press_data.get("date")
                            if press_date and (sec_date is None or press_date > sec_date):
                                combined_metrics[label] = {
                                    "value": val_str,
                                    "source": "Press Release",
                                    "date": press_date
                                }

                # --- Display combined result ---
                if not combined_metrics:
                    st.info("No financial metrics available from SEC or press release.")
                else:
                    for label in sorted(combined_metrics.keys()):
                        info = combined_metrics[label]
                        val = info["value"]
                        source = info["source"]
                        date = info["date"]
                        if source == "Press Release" and press_data and press_data.get("url"):
                            formatted_date = f""" _(from {source}, {date}, <a href="{press_data['url']}" target="_blank">View Release</a>)_"""
                        elif source == "SEC Filing" and inline_url:
                            formatted_date = f""" _(from {source}, {date}, <a href="{inline_url}" target="_blank">View Filing</a>)_"""
                        else:
                            formatted_date = f" _(from {source}, {date})_" if date else f" _(from {source})_"

                        # Format value
                        if isinstance(val, (int, float)):
                            if "Bitcoin" in label:
                                val_display = f"{val:,.0f} BTC"
                            else:
                                val_display = f"${val:,.2f}"
                        else:
                            val_display = val

                        st.markdown(f"- **{label}**: {val_display}{formatted_date}", unsafe_allow_html=True)
